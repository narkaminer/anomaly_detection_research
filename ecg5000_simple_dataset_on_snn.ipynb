{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"58yBd92gdlHs","executionInfo":{"status":"ok","timestamp":1747522743369,"user_tz":-180,"elapsed":17712,"user":{"displayName":"saccidÄnanda","userId":"10600434951760663586"}},"outputId":"9f86e3b0-e1d4-45ae-c958-f71fb490ebf9"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/374.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hEpoch 1, Loss: 3.7646\n","Epoch 2, Loss: 2.7482\n","Epoch 3, Loss: 2.9723\n","Epoch 4, Loss: 2.8704\n","Epoch 5, Loss: 2.9678\n","Epoch 6, Loss: 2.9028\n","Epoch 7, Loss: 2.9352\n","Epoch 8, Loss: 2.8052\n","Epoch 9, Loss: 2.9677\n","Epoch 10, Loss: 2.8379\n","              precision    recall  f1-score   support\n","\n","         0.0     1.0000    1.0000    1.0000        58\n","         1.0     1.0000    1.0000    1.0000        42\n","\n","    accuracy                         1.0000       100\n","   macro avg     1.0000    1.0000    1.0000       100\n","weighted avg     1.0000    1.0000    1.0000       100\n","\n"]}],"source":["# ðŸ”§ Ð£ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ° Ð½ÐµÐ¾Ð±Ñ…Ð¾Ð´Ð¸Ð¼Ñ‹Ñ… Ð±Ð¸Ð±Ð»Ð¸Ð¾Ñ‚ÐµÐº\n","!pip install snntorch tslearn --quiet\n","\n","# ðŸ“š Ð˜Ð¼Ð¿Ð¾Ñ€Ñ‚Ñ‹\n","import torch\n","import torch.nn as nn\n","import snntorch as snn\n","from snntorch import surrogate\n","from snntorch import functional as SF\n","from snntorch import utils\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import classification_report\n","import numpy as np\n","\n","# ðŸ“Š Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° ECG5000 (Ð°Ð½Ð°Ð»Ð¸Ð· Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ñ… Ñ€ÑÐ´Ð¾Ð²)\n","from tslearn.datasets import UCR_UEA_datasets\n","ucr = UCR_UEA_datasets()\n","X, y, _, _ = ucr.load_dataset(\"ECG5000\")\n","\n","# ðŸ“Š Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° ECG5000 (Ð°Ð½Ð°Ð»Ð¸Ð· Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ñ… Ñ€ÑÐ´Ð¾Ð²)\n","from tslearn.datasets import UCR_UEA_datasets\n","ucr = UCR_UEA_datasets()\n","X, y, _, _ = ucr.load_dataset(\"ECG5000\")\n","\n","# ÐŸÑ€ÐµÐ¾Ð±Ñ€Ð°Ð·ÑƒÐµÐ¼ Ðº Ð½ÑƒÐ¶Ð½Ð¾Ð¼Ñƒ Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ñƒ\n","X = X.squeeze()\n","y = y.astype(int)\n","\n","# âš™ï¸ Ð‘Ð¸Ð½Ð°Ñ€Ð½Ð°Ñ ÐºÐ»Ð°ÑÑÐ¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ñ: 1 = Ð½Ð¾Ñ€Ð¼Ð°, Ð¾ÑÑ‚Ð°Ð»ÑŒÐ½Ñ‹Ðµ = Ð°Ð½Ð¾Ð¼Ð°Ð»Ð¸Ñ\n","y_binary = (y != 1).astype(int)\n","\n","# âš–ï¸ ÐÐ¾Ñ€Ð¼Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","# ðŸ”€ Ð Ð°Ð·Ð´ÐµÐ»ÐµÐ½Ð¸Ðµ\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X_scaled, y_binary, test_size=0.2, random_state=42, stratify=y_binary\n",")\n","\n","# ðŸ§  ÐŸÑ€ÐµÐ¾Ð±Ñ€Ð°Ð·Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð² Ñ‚ÐµÐ½Ð·Ð¾Ñ€Ñ‹\n","X_train = torch.tensor(X_train, dtype=torch.float32)\n","X_test = torch.tensor(X_test, dtype=torch.float32)\n","y_train = torch.tensor(y_train, dtype=torch.float32)\n","y_test = torch.tensor(y_test, dtype=torch.float32)\n","\n","# Ð”Ð¾Ð±Ð°Ð²Ð¸Ð¼ Ñ€Ð°Ð·Ð¼ÐµÑ€Ð½Ð¾ÑÑ‚ÑŒ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð¸: Ð¿Ñ€Ð¾ÑÑ‚Ð¾ Ð´ÑƒÐ±Ð»Ð¸Ñ€ÑƒÐµÐ¼ Ð²Ñ…Ð¾Ð´\n","time_steps = 25\n","X_train_seq = X_train.unsqueeze(1).repeat(1, time_steps, 1)\n","X_test_seq = X_test.unsqueeze(1).repeat(1, time_steps, 1)\n","\n","# ðŸ“¦ DataLoader\n","train_data = torch.utils.data.TensorDataset(X_train_seq, y_train)\n","test_data = torch.utils.data.TensorDataset(X_test_seq, y_test)\n","train_loader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\n","test_loader = torch.utils.data.DataLoader(test_data, batch_size=64, shuffle=False)\n","\n","# ðŸ”§ SNN Ð¼Ð¾Ð´ÐµÐ»ÑŒ\n","class SNN(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.fc1 = nn.Linear(X_train.shape[1], 64)\n","        self.lif1 = snn.Leaky(beta=0.9, spike_grad=surrogate.fast_sigmoid())\n","        self.fc2 = nn.Linear(64, 1)\n","        self.lif2 = snn.Leaky(beta=0.9, spike_grad=surrogate.fast_sigmoid())\n","\n","    def forward(self, x):\n","        mem1 = self.lif1.init_leaky()\n","        mem2 = self.lif2.init_leaky()\n","        spk2_rec = []\n","\n","        for step in range(x.size(1)):\n","            cur1 = self.fc1(x[:, step])\n","            spk1, mem1 = self.lif1(cur1, mem1)\n","\n","            cur2 = self.fc2(spk1)\n","            spk2, mem2 = self.lif2(cur2, mem2)\n","            spk2_rec.append(spk2)\n","\n","        return torch.stack(spk2_rec, dim=1).sum(1)\n","\n","# âš™ï¸ Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð¼Ð¾Ð´ÐµÐ»Ð¸\n","model = SNN()\n","loss_fn = nn.BCEWithLogitsLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n","\n","# ðŸŽ¯ ÐžÐ±ÑƒÑ‡ÐµÐ½Ð¸Ðµ\n","for epoch in range(10):\n","    total_loss = 0\n","    for x, y in train_loader:\n","        out = model(x).squeeze()\n","        loss = loss_fn(out, y)\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n","\n","# âœ… Ð¢ÐµÑÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ\n","model.eval()\n","all_preds = []\n","all_labels = []\n","\n","with torch.no_grad():\n","    for x, y in test_loader:\n","        out = model(x).squeeze()\n","        preds = (torch.sigmoid(out) > 0.5).int()\n","        all_preds.extend(preds.tolist())\n","        all_labels.extend(y.tolist())\n","\n","# ðŸ“Š ÐœÐµÑ‚Ñ€Ð¸ÐºÐ¸\n","print(classification_report(all_labels, all_preds, digits=4))\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"authorship_tag":"ABX9TyNZT5r3sQZy9OETg7Uk1QwR"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}